<!DOCTYPE HTML>

<html>
	<head>
		<title>Pyhton - collecte</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							
							
							<header id="header">
									<a href="index.html" class="logo"><strong>Accueil</strong></a>
									<ul class="icons">
										<li><a href="https://www.linkedin.com/in/hugo-fromont/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
										<li><a href="https://github.com/HugoFromont" class="icon brands fa-github"><span class="label">Github</span></a></li>
									</ul>
								</header>

							



							<!-- Content -->
								

<section>

										<h1>Collecte des données avec Python</h1>

<h2 id="importerunfichercsv">Importer un ficher CSV</h2>

<p>Pour importer un fichier csv, nous utilisons la commande read_csv de la library Pandas</p>

<pre><code class="{python} language-{python}">import pandas as pd
df = pd.read_csv("chemin/data.csv")
</code></pre>

<p>Grâce à la fonction précédante nous avons stocké notre CSV dans un DataFrame. Nous verrons plus loin comment manipuler les données d'un DataFrame</p>

<h2 id="importerunfichierhdf5">Importer un fichier hdf5</h2>

<p>Avec la même librairie il est possible d'importer des fichier hdf5</p>

<pre><code class="{python} language-{python}">df = pd.read_hdf('chemin/data.hdf5')
</code></pre>

<h2 id="importerunfichierdbf">Importer un fichier dbf</h2>

<p>pour importer un fichier dbf, nous utilisons la librarie dbf</p>

<pre><code class="{python} language-{python}">import dbf
table = dbf.Table('chemin.dbf')
</code></pre>

<p>nous pouvons ensuite naviguer dans la table à l'aide du code suivant</p>

<pre><code class="{python} language-{python}">table.open()
for ligne in table:
    print(ligne.variable1,ligne.variable2)
table.close()
</code></pre>

<h2 id="connectionunedatabasemysql">Connection à une DataBase MySQL</h2>

<p>Avec python, il est possible d'établir une connection avec une base de données MySQL, et d'effectuer tout type d'opération (Create Table, Insert into, SELECT ...). Pour cela,  nous devons d'abord connecter python à une DatBase de MySQL.</p>

<pre><code class="{python} language-{python}">import mysql.connector

conn = mysql.connector.connect(
  host="localhost",
  user="root",
  passwd="#########",
  database="DataBase_Name"
)

cursor = conn.cursor()
</code></pre>

<p>Nous pouvons maintenant éxécuter des requètes SQL.
Par exemple pour créer une Table dans notre DataBase, nous pouvons utiliser la commande suivante :</p>

<pre><code class="{python} language-{python}">cursor.execute("""
    CREATE TABLE IF NOT EXISTS nom_table (
        id_user int(5) NOT NULL AUTO_INCREMENT,
        user_name varchar(40) NOT NULL,
        user_age int(3) DEFAULT NULL,
        PRIMARY KEY(id_user)
    );
    """)
</code></pre>

<p>Nous pouvons ensuite insérer des données dans la Table. 
Voici une fonction qui permet d'insérer des données dans la table créée au dessus. Cette fonction utilise la fonction try afin de savoir si l'opération à été correctement éxécutée.</p>

<pre><code class="{python} language-{python}">def insert_users(data):
    query = 'INSERT INTO nom_table(user_name,user_age) '\
    'VALUES(%s,"%i");'%(data)
    try:
        cursor.execute(query)
        conn.commit()
        print('successful insertion')
    except :
        print('failled insertion')

insert_users(('Hugo',22))
</code></pre>

<p>Enfin nous pouvons bien évidement récupérer les informations stockées dans la base de données. Voici comment réaliser cette opération  </p>

<pre><code class="{python} language-{python}">cursor.execute("""SELECT * from nom_table""")
rows = cursor.fetchall()
print(rows)
</code></pre>

<h2 id="rcuprerlesdonnesdelapitwitter">Récupérer les données de l'API Twitter</h2>

<p>Twitter dispose d'une API avec laquelle il est possible d'intéragir (récupérer des tweets, publier des tweets...).
Afin  de récupérer des données Twitter, Twitter nous demande de nous créer un compte développeur <a href="https://developer.twitter.com/">(Twitter Developer Platform)</a>
Une fois le compte créé, nous disposons d'identifiants qui nous permettent de nous connecter à l'API.
Grâce à ces identifiants, nous allons configurer notre connection à l'API</p>

<pre><code class="{python} language-{python}">import tweepy

access_token =        "##########################################"
access_token_secret = "##########################################"
consumer_key =        "##########################################"
consumer_secret =     "##########################################"

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)
</code></pre>

<p>Nous pouvons maintenant faire une recherche par mot clef pour récupérer des tweets précis</p>

<pre><code class="{python} language-{python}">query = "#python"
max_tweets = 100
public_tweets = tweepy.Cursor(api.search, q=query).items(max_tweets)
for tweet in public_tweets:
    user_name = tweet.user.name
    followers_count = tweet.user.followers_count
    location = tweet.user.location
    id_tweet = tweet.id
    text = tweet.text
    lang = tweet.lang
    date = tweet.created_at
    nb_retweet = tweet.retweet_count
    nb_likes = tweet.favorite_count
    print(text)
</code></pre>

<p>Le code au dessus permet d'afficher 100 tweets qui contiennent le hashtag "python". Il est possible de sauvegarder ces informations dans un DataFrame, vecteur...
De nombreuses informations peuvent être récupérées, une liste complète est disponible sur le site de <a href="https://developer.twitter.com/">(Twitter Developer Platform)</a>. Quelques exemples des informations qu'il est possible de récupérer sont disponibles dans le code au dessus.</p>

<h2 id="collectededonneslaidedescrappingweb">Collecte de données à l'aide de Scrapping Web</h2>

<p>Avec python il est possible d'utiliser des techiniques de Scrapping afin de récupérer les données d'une page Web.</p>

<h3 id="webscrapingavecdriver">Web-Scraping avec driver</h3>

<p>Dans un premier temps nous allons voir comment il est possible d'utiliser un driver pour naviguer sur une page Web.
Tout d'abord il est nécessaire de télécharger un driver pour le moteur de recherche que vous souhaitez utiliser. Et de le configurer pour pouvoir l'utiliser dans un script python. Sur internet, il existe de nombreurses documentations sur le sujet.</p>

<p>Une fois votre driver installé, nous allons utiliser la librarie Selenium pour controler notre driver.</p>

<pre><code class="{python} language-{python}">from selenium import webdriver
import os
</code></pre>

<p>Nous devons renseigner l'emplacement du driver avec la commande suivante et indiquer l'url que le driver doit ouvrir, ici j'ai pris l'url du film inception sur allociné</p>

<pre><code class="{python} language-{python}">os.environ["PATH"] = "C:/WebScraping/"
url = "http://www.allocine.fr/film/fichefilm_gen_cfilm=143692.html"
</code></pre>

<p>Nous allons maintenant dire à notre driver d'ouvrir la page web demandée</p>

<pre><code class="{python} language-{python}">driver = webdriver.Chrome()
driver.get(url)
headers = {}
headers['User-Agent'] = "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) 
Chrome/41.0.2228.0 Safari/537.36"
</code></pre>

<p>Nous pouvons maintenant récupérer un élément de la page web avec du language Xpath</p>

<pre><code class="{python} language-{python}">html_element = driver.find_elements_by_xpath('language_Xpath')
for element in html_element:
    print(element.get_attribute('innerHTML'))
</code></pre>

<p>Mais nous pouvons aussi naviguer au sein de la page web et même cliquer sur des éléments qui ont des liens url.
exemple du code pour scroller la page web </p>

<pre><code class="{python} language-{python}">driver.execute_script("window.scrollBy(0, 1000000)")
</code></pre>

<p>exemple d'une commande qui permet de cliquer sur le bouton d'une page web</p>

<pre><code class="{python} language-{python}">driver.find_element_by_xpath("chemin_du_boutton_en_Xpath").click()
</code></pre>

<p>Grâce à cette technique, il est possible de récupérer tout type de données (text, images...)</p>

<h3 id="webscrapingaveclalibrarybeautifulsoup">Web-Scraping avec la library BeautifulSoup</h3>

<p>Voici une deuxième méthode de Scapping qui ne nécessite pas de driver. Et qui est plus simple à utiliser.
Nous allons dans un premier temps récupérer le code source d'une page html avec la commande suivante</p>

<pre><code class="{python} language-{python}">from requests import get
url = "http://www.allocine.fr/film/fichefilm_gen_cfilm=143692.html"
code_source = get(url)
</code></pre>

<p>Puis grâce à la librarie BeautifulSoup nous allons pouvoir récupérer les informations qui nous intéressent</p>

<pre><code class="{python} language-{python}">from bs4 import BeautifulSoup
object_BeautifulSoup = BeautifulSoup(code_source.text, 'html.parser')
element = object_BeautifulSoup.find_all('div', class_ = 'name_class')
result = element.h2
print(resulat)
</code></pre>

<h2 id="importerunetablebigquery">Importer une table BigQuery</h2>

<p>Pour récperer le resultats d'une requete BigQuery, il est possible d'utiliser l'API de google. Pour ce faire il est necessaire disposer une clef API. Sur la console de Google Cloud Platform il est possible de creér un compte service pour disposer de droit (IAM -> comptes de services).</p>

<pre><code class="{python} language-{python}">from google.cloud import bigquery
client = bigquery.Client()

sql = (
    "SELECT * FROM `project_id.dataset_id.table_id` LIMIT 10"
)

data = client.query(sql).to_dataframe()
</code></pre>																	

				
									
									
									
									
									
								</section>	
									
									
									
									
									
									
									



						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li>
											<span class="opener">Pyhton</span>
											<ul>
												<li><a href="Python_collecte.html">Collecte des données</a></li>
												<li><a href="Python_manipulation.html">Manipuation de données</a></li>
												<li><a href="Python_visualisation.html">Visualisation</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">R</span>
											<ul>
												<li><a href="R_collecte.html">Collecte des données</a></li>
												<li><a href="R_manipulation.html">Manipuation de données</a></li>
												<li><a href="R_visualisation.html">Visualisation</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">Projet Python</span>
											<ul>
												<li><a href="CNN_chien_chat.html">CNN - Classification chiens chats </a></li>
												<li><a href="CNN_MNIST.html">CNN - MNIST </a></li>
												<li><a href="NLP_election.html">NLP - Election présidentielle</a></li>
												<li><a href="CNN_Cancer_detection.html">CNN - Detection du cancer </a></li>
												<li><a href="#">CNN - Quick Draw </a></li>
											</ul>
										</li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Contact</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">hugo.fromont35@gmail.com</a></li>
										<li class="icon brands fa-linkedin"><a href="https://www.linkedin.com/in/hugo-fromont/">Linkedin</a></li>
										<li class="icon brands fa-github"><a href="https://github.com/HugoFromont">Github</a></li>
									</ul>
								</section>
								
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Untitled. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>


						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>